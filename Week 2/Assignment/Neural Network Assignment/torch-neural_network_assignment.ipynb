{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomerBartDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for label, sub_dir in enumerate(os.listdir(root_dir)):\n",
    "            sub_dir_path = os.path.join(root_dir, sub_dir)\n",
    "            for img_name in os.listdir(sub_dir_path):\n",
    "                self.image_paths.append(os.path.join(sub_dir_path, img_name))\n",
    "                self.labels.append(label)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HomerBartDataset(root_dir='homer_bart', transform=transform)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(64 * 64 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6723271682858467\n",
      "Epoch 2, Loss: 0.6326306983828545\n",
      "Epoch 3, Loss: 0.6095225661993027\n",
      "Epoch 4, Loss: 0.5822339504957199\n",
      "Epoch 5, Loss: 0.588665209710598\n",
      "Epoch 6, Loss: 0.5684722438454628\n",
      "Epoch 7, Loss: 0.5648433417081833\n",
      "Epoch 8, Loss: 0.5521713979542255\n",
      "Epoch 9, Loss: 0.540608361363411\n",
      "Epoch 10, Loss: 0.5518081001937389\n",
      "Epoch 11, Loss: 0.5291669443249702\n",
      "Epoch 12, Loss: 0.5186579376459122\n",
      "Epoch 13, Loss: 0.5163976848125458\n",
      "Epoch 14, Loss: 0.5348396189510822\n",
      "Epoch 15, Loss: 0.5069563388824463\n",
      "Epoch 16, Loss: 0.4986739084124565\n",
      "Epoch 17, Loss: 0.504387617111206\n",
      "Epoch 18, Loss: 0.4976184666156769\n",
      "Epoch 19, Loss: 0.476128164678812\n",
      "Epoch 20, Loss: 0.5032254606485367\n",
      "Epoch 21, Loss: 0.47716081887483597\n",
      "Epoch 22, Loss: 0.4847048707306385\n",
      "Epoch 23, Loss: 0.4728654697537422\n",
      "Epoch 24, Loss: 0.46974874660372734\n",
      "Epoch 25, Loss: 0.47473715618252754\n",
      "Epoch 26, Loss: 0.4643835462629795\n",
      "Epoch 27, Loss: 0.4581010974943638\n",
      "Epoch 28, Loss: 0.45890340954065323\n",
      "Epoch 29, Loss: 0.44505126774311066\n",
      "Epoch 30, Loss: 0.4483342915773392\n",
      "Epoch 31, Loss: 0.4402935318648815\n",
      "Epoch 32, Loss: 0.47331223636865616\n",
      "Epoch 33, Loss: 0.4414241686463356\n",
      "Epoch 34, Loss: 0.44824185222387314\n",
      "Epoch 35, Loss: 0.427640825510025\n",
      "Epoch 36, Loss: 0.4266415797173977\n",
      "Epoch 37, Loss: 0.4156593158841133\n",
      "Epoch 38, Loss: 0.4310840293765068\n",
      "Epoch 39, Loss: 0.4195019416511059\n",
      "Epoch 40, Loss: 0.4099520072340965\n",
      "Epoch 41, Loss: 0.408402718603611\n",
      "Epoch 42, Loss: 0.41879864037036896\n",
      "Epoch 43, Loss: 0.40827494859695435\n",
      "Epoch 44, Loss: 0.3970630019903183\n",
      "Epoch 45, Loss: 0.3896232917904854\n",
      "Epoch 46, Loss: 0.3915209211409092\n",
      "Epoch 47, Loss: 0.3905440829694271\n",
      "Epoch 48, Loss: 0.40826351940631866\n",
      "Epoch 49, Loss: 0.422726534307003\n",
      "Epoch 50, Loss: 0.39361128956079483\n",
      "Epoch 51, Loss: 0.3849039040505886\n",
      "Epoch 52, Loss: 0.40036337077617645\n",
      "Epoch 53, Loss: 0.41569890454411507\n",
      "Epoch 54, Loss: 0.4368656687438488\n",
      "Epoch 55, Loss: 0.41942860558629036\n",
      "Epoch 56, Loss: 0.41133173927664757\n",
      "Epoch 57, Loss: 0.3936249576508999\n",
      "Epoch 58, Loss: 0.377564512193203\n",
      "Epoch 59, Loss: 0.372696403414011\n",
      "Epoch 60, Loss: 0.3684702031314373\n",
      "Epoch 61, Loss: 0.36130432039499283\n",
      "Epoch 62, Loss: 0.36144454404711723\n",
      "Epoch 63, Loss: 0.366185262799263\n",
      "Epoch 64, Loss: 0.3724842518568039\n",
      "Epoch 65, Loss: 0.3772047236561775\n",
      "Epoch 66, Loss: 0.3884042240679264\n",
      "Epoch 67, Loss: 0.3803868591785431\n",
      "Epoch 68, Loss: 0.37248145043849945\n",
      "Epoch 69, Loss: 0.36611727997660637\n",
      "Epoch 70, Loss: 0.36967820301651955\n",
      "Epoch 71, Loss: 0.36082881316542625\n",
      "Epoch 72, Loss: 0.3583429716527462\n",
      "Epoch 73, Loss: 0.3472404479980469\n",
      "Epoch 74, Loss: 0.35015755146741867\n",
      "Epoch 75, Loss: 0.3496755063533783\n",
      "Epoch 76, Loss: 0.34572284668684006\n",
      "Epoch 77, Loss: 0.34107526391744614\n",
      "Epoch 78, Loss: 0.34215936437249184\n",
      "Epoch 79, Loss: 0.34298066794872284\n",
      "Epoch 80, Loss: 0.341991413384676\n",
      "Epoch 81, Loss: 0.337373323738575\n",
      "Epoch 82, Loss: 0.34205861017107964\n",
      "Epoch 83, Loss: 0.34228212758898735\n",
      "Epoch 84, Loss: 0.34017377346754074\n",
      "Epoch 85, Loss: 0.33442701026797295\n",
      "Epoch 86, Loss: 0.33567338436841965\n",
      "Epoch 87, Loss: 0.3342263624072075\n",
      "Epoch 88, Loss: 0.3358456939458847\n",
      "Epoch 89, Loss: 0.33501701056957245\n",
      "Epoch 90, Loss: 0.3329842761158943\n",
      "Epoch 91, Loss: 0.3306174501776695\n",
      "Epoch 92, Loss: 0.33073027804493904\n",
      "Epoch 93, Loss: 0.333717480301857\n",
      "Epoch 94, Loss: 0.3333110846579075\n",
      "Epoch 95, Loss: 0.33763886988162994\n",
      "Epoch 96, Loss: 0.33338574692606926\n",
      "Epoch 97, Loss: 0.3311603255569935\n",
      "Epoch 98, Loss: 0.33561747893691063\n",
      "Epoch 99, Loss: 0.32952839881181717\n",
      "Epoch 100, Loss: 0.3286214545369148\n",
      "Epoch 101, Loss: 0.33032019808888435\n",
      "Epoch 102, Loss: 0.32750096917152405\n",
      "Epoch 103, Loss: 0.3280705586075783\n",
      "Epoch 104, Loss: 0.3273276910185814\n",
      "Epoch 105, Loss: 0.3271790072321892\n",
      "Epoch 106, Loss: 0.3264893740415573\n",
      "Epoch 107, Loss: 0.3261372782289982\n",
      "Epoch 108, Loss: 0.3268398642539978\n",
      "Epoch 109, Loss: 0.3265484422445297\n",
      "Epoch 110, Loss: 0.32730238139629364\n",
      "Epoch 111, Loss: 0.32654261589050293\n",
      "Epoch 112, Loss: 0.3255596272647381\n",
      "Epoch 113, Loss: 0.3253277689218521\n",
      "Epoch 114, Loss: 0.3254020996391773\n",
      "Epoch 115, Loss: 0.3281497359275818\n",
      "Epoch 116, Loss: 0.32484113425016403\n",
      "Epoch 117, Loss: 0.32509515434503555\n",
      "Epoch 118, Loss: 0.32480741664767265\n",
      "Epoch 119, Loss: 0.3280118480324745\n",
      "Epoch 120, Loss: 0.3251121938228607\n",
      "Epoch 121, Loss: 0.3249417245388031\n",
      "Epoch 122, Loss: 0.32475802302360535\n",
      "Epoch 123, Loss: 0.324334230273962\n",
      "Epoch 124, Loss: 0.32448335736989975\n",
      "Epoch 125, Loss: 0.3246051035821438\n",
      "Epoch 126, Loss: 0.32405905053019524\n",
      "Epoch 127, Loss: 0.3243698216974735\n",
      "Epoch 128, Loss: 0.32458222284913063\n",
      "Epoch 129, Loss: 0.3243967667222023\n",
      "Epoch 130, Loss: 0.3243953064084053\n",
      "Epoch 131, Loss: 0.32450995966792107\n",
      "Epoch 132, Loss: 0.32716038078069687\n",
      "Epoch 133, Loss: 0.3240272104740143\n",
      "Epoch 134, Loss: 0.3239166736602783\n",
      "Epoch 135, Loss: 0.3266197592020035\n",
      "Epoch 136, Loss: 0.3235955946147442\n",
      "Epoch 137, Loss: 0.3234095089137554\n",
      "Epoch 138, Loss: 0.32352080196142197\n",
      "Epoch 139, Loss: 0.32336778193712234\n",
      "Epoch 140, Loss: 0.32337842509150505\n",
      "Epoch 141, Loss: 0.3263270817697048\n",
      "Epoch 142, Loss: 0.3232908807694912\n",
      "Epoch 143, Loss: 0.3231923133134842\n",
      "Epoch 144, Loss: 0.32318125665187836\n",
      "Epoch 145, Loss: 0.32332299277186394\n",
      "Epoch 146, Loss: 0.3261091373860836\n",
      "Epoch 147, Loss: 0.3262139596045017\n",
      "Epoch 148, Loss: 0.3230278193950653\n",
      "Epoch 149, Loss: 0.32295623421669006\n",
      "Epoch 150, Loss: 0.32292043790221214\n",
      "Epoch 151, Loss: 0.3228314481675625\n",
      "Epoch 152, Loss: 0.3228065073490143\n",
      "Epoch 153, Loss: 0.3258349783718586\n",
      "Epoch 154, Loss: 0.3227766752243042\n",
      "Epoch 155, Loss: 0.32280150800943375\n",
      "Epoch 156, Loss: 0.3226986676454544\n",
      "Epoch 157, Loss: 0.32275548949837685\n",
      "Epoch 158, Loss: 0.3226364627480507\n",
      "Epoch 159, Loss: 0.3226448893547058\n",
      "Epoch 160, Loss: 0.32261980697512627\n",
      "Epoch 161, Loss: 0.3226010724902153\n",
      "Epoch 162, Loss: 0.3225054256618023\n",
      "Epoch 163, Loss: 0.32248587906360626\n",
      "Epoch 164, Loss: 0.3224464990198612\n",
      "Epoch 165, Loss: 0.32249775156378746\n",
      "Epoch 166, Loss: 0.3224048689007759\n",
      "Epoch 167, Loss: 0.3224012516438961\n",
      "Epoch 168, Loss: 0.32237324491143227\n",
      "Epoch 169, Loss: 0.3224131725728512\n",
      "Epoch 170, Loss: 0.3223729468882084\n",
      "Epoch 171, Loss: 0.3223237097263336\n",
      "Epoch 172, Loss: 0.32539964094758034\n",
      "Epoch 173, Loss: 0.32533420622348785\n",
      "Epoch 174, Loss: 0.32226572558283806\n",
      "Epoch 175, Loss: 0.32227274402976036\n",
      "Epoch 176, Loss: 0.32224883511662483\n",
      "Epoch 177, Loss: 0.32222653925418854\n",
      "Epoch 178, Loss: 0.32218582183122635\n",
      "Epoch 179, Loss: 0.32217812165617943\n",
      "Epoch 180, Loss: 0.32519542798399925\n",
      "Epoch 181, Loss: 0.3221665434539318\n",
      "Epoch 182, Loss: 0.32214946672320366\n",
      "Epoch 183, Loss: 0.3221357502043247\n",
      "Epoch 184, Loss: 0.32214895263314247\n",
      "Epoch 185, Loss: 0.3220873363316059\n",
      "Epoch 186, Loss: 0.32207342609763145\n",
      "Epoch 187, Loss: 0.3220815807580948\n",
      "Epoch 188, Loss: 0.3221023827791214\n",
      "Epoch 189, Loss: 0.3251314088702202\n",
      "Epoch 190, Loss: 0.32205822691321373\n",
      "Epoch 191, Loss: 0.3280920647084713\n",
      "Epoch 192, Loss: 0.32200171798467636\n",
      "Epoch 193, Loss: 0.32199352979660034\n",
      "Epoch 194, Loss: 0.3219526410102844\n",
      "Epoch 195, Loss: 0.3250127583742142\n",
      "Epoch 196, Loss: 0.324994970113039\n",
      "Epoch 197, Loss: 0.3250008448958397\n",
      "Epoch 198, Loss: 0.32190579921007156\n",
      "Epoch 199, Loss: 0.32191964238882065\n",
      "Epoch 200, Loss: 0.3219069242477417\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.5925925925926%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
