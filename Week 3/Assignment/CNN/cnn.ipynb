{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheetah=\"Cheetah\"\n",
    "crocodile=\"Crocodile\"\n",
    "elephants=\"Elephants\"\n",
    "tiger=\"Tiger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animals(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for label, sub_dir in enumerate(os.listdir(root_dir)):\n",
    "            sub_dir_path = os.path.join(root_dir, sub_dir)\n",
    "            for img_name in os.listdir(sub_dir_path):\n",
    "                self.image_paths.append(os.path.join(sub_dir_path, img_name))\n",
    "                self.labels.append(label)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Animals(root_dir='datafiles', transform=transform)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of pretrained model made the accuracy good and reasonable . I tried using normal neural network but it was taking a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(64 * 64 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here\n",
    "        return x\n",
    "\n",
    "normalmodel = SimpleNN()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(normalmodel.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look for normal model and then we will use pretrained model to see the effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aditi\\miniconda3\\envs\\project\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3658410894109847\n",
      "Epoch 2, Loss: 1.3307666575655024\n",
      "Epoch 3, Loss: 1.2905808129209153\n",
      "Epoch 4, Loss: 1.2604636182176305\n",
      "Epoch 5, Loss: 1.2374071009615635\n",
      "Epoch 6, Loss: 1.2170356410614989\n",
      "Epoch 7, Loss: 1.20180895227067\n",
      "Epoch 8, Loss: 1.1877186552007148\n",
      "Epoch 9, Loss: 1.169372208574985\n",
      "Epoch 10, Loss: 1.1535945978570492\n",
      "Epoch 11, Loss: 1.1396273907194747\n",
      "Epoch 12, Loss: 1.1310526627175352\n",
      "Epoch 13, Loss: 1.1055379662107914\n",
      "Epoch 14, Loss: 1.1012491875506463\n",
      "Epoch 15, Loss: 1.0946863293647766\n",
      "Epoch 16, Loss: 1.0700654641110847\n",
      "Epoch 17, Loss: 1.0599412436180926\n",
      "Epoch 18, Loss: 1.0470665959601706\n",
      "Epoch 19, Loss: 1.0290153140717364\n",
      "Epoch 20, Loss: 1.0108712340923065\n",
      "Epoch 21, Loss: 1.0004297025660251\n",
      "Epoch 22, Loss: 0.9919421989867028\n",
      "Epoch 23, Loss: 0.9778285216777882\n",
      "Epoch 24, Loss: 0.9676223587482533\n",
      "Epoch 25, Loss: 0.9597525964391992\n",
      "Epoch 26, Loss: 0.9487128004114679\n",
      "Epoch 27, Loss: 0.9288129058290036\n",
      "Epoch 28, Loss: 0.9284881150468867\n",
      "Epoch 29, Loss: 0.9060145299485389\n",
      "Epoch 30, Loss: 0.9033200360359029\n",
      "Epoch 31, Loss: 0.8774257644693902\n",
      "Epoch 32, Loss: 0.8736723598013533\n",
      "Epoch 33, Loss: 0.8642864950159763\n",
      "Epoch 34, Loss: 0.862795558381588\n",
      "Epoch 35, Loss: 0.8440885112640706\n",
      "Epoch 36, Loss: 0.8379146088945105\n",
      "Epoch 37, Loss: 0.8115426327319856\n",
      "Epoch 38, Loss: 0.8124263476818165\n",
      "Epoch 39, Loss: 0.792701724995958\n",
      "Epoch 40, Loss: 0.7912137888847514\n",
      "Epoch 41, Loss: 0.7868176647957336\n",
      "Epoch 42, Loss: 0.7667751261528503\n",
      "Epoch 43, Loss: 0.7532352259818543\n",
      "Epoch 44, Loss: 0.7486255245005831\n",
      "Epoch 45, Loss: 0.7427962194097802\n",
      "Epoch 46, Loss: 0.7270627941222901\n",
      "Epoch 47, Loss: 0.7149364123953149\n",
      "Epoch 48, Loss: 0.7007040578000089\n",
      "Epoch 49, Loss: 0.6887424835499297\n",
      "Epoch 50, Loss: 0.6918394508513999\n",
      "Epoch 51, Loss: 0.6722185700497729\n",
      "Epoch 52, Loss: 0.6649086050530697\n",
      "Epoch 53, Loss: 0.6623276662319264\n",
      "Epoch 54, Loss: 0.6485347246870081\n",
      "Epoch 55, Loss: 0.6385187278402612\n",
      "Epoch 56, Loss: 0.6618245776663435\n",
      "Epoch 57, Loss: 0.6257735979049763\n",
      "Epoch 58, Loss: 0.6014882319785179\n",
      "Epoch 59, Loss: 0.595571608619487\n",
      "Epoch 60, Loss: 0.5815528231732389\n",
      "Epoch 61, Loss: 0.5941162198147876\n",
      "Epoch 62, Loss: 0.5752045036630428\n",
      "Epoch 63, Loss: 0.5612141559732721\n",
      "Epoch 64, Loss: 0.5494294883088863\n",
      "Epoch 65, Loss: 0.5349445171812748\n",
      "Epoch 66, Loss: 0.5266380303717674\n",
      "Epoch 67, Loss: 0.5302321520891595\n",
      "Epoch 68, Loss: 0.5245119523494801\n",
      "Epoch 69, Loss: 0.5221146383184068\n",
      "Epoch 70, Loss: 0.4908009127099463\n",
      "Epoch 71, Loss: 0.4955492653745286\n",
      "Epoch 72, Loss: 0.48283200378113605\n",
      "Epoch 73, Loss: 0.4773384877975951\n",
      "Epoch 74, Loss: 0.4651849999072704\n",
      "Epoch 75, Loss: 0.4594140629819099\n",
      "Epoch 76, Loss: 0.44401672419081345\n",
      "Epoch 77, Loss: 0.4437582169441467\n",
      "Epoch 78, Loss: 0.4351817122165193\n",
      "Epoch 79, Loss: 0.422287457800926\n",
      "Epoch 80, Loss: 0.4232429877874699\n",
      "Epoch 81, Loss: 0.4037287799601859\n",
      "Epoch 82, Loss: 0.4113841519710865\n",
      "Epoch 83, Loss: 0.3914845630209497\n",
      "Epoch 84, Loss: 0.39453834453795816\n",
      "Epoch 85, Loss: 0.37957766335061255\n",
      "Epoch 86, Loss: 0.36932855559156297\n",
      "Epoch 87, Loss: 0.36018695698139513\n",
      "Epoch 88, Loss: 0.35103031041774346\n",
      "Epoch 89, Loss: 0.3518409767049424\n",
      "Epoch 90, Loss: 0.35104191969049736\n",
      "Epoch 91, Loss: 0.3412795425095457\n",
      "Epoch 92, Loss: 0.351131057802667\n",
      "Epoch 93, Loss: 0.3208269533958841\n",
      "Epoch 94, Loss: 0.3313351352798178\n",
      "Epoch 95, Loss: 0.3225771099963087\n",
      "Epoch 96, Loss: 0.30968866449721316\n",
      "Epoch 97, Loss: 0.30184732567756734\n",
      "Epoch 98, Loss: 0.28615960986056227\n",
      "Epoch 99, Loss: 0.2950636975942774\n",
      "Epoch 100, Loss: 0.28116044338713303\n",
      "Epoch 101, Loss: 0.2741461389876427\n",
      "Epoch 102, Loss: 0.26485122486631923\n",
      "Epoch 103, Loss: 0.26935247950097346\n",
      "Epoch 104, Loss: 0.27638510567076663\n",
      "Epoch 105, Loss: 0.2731063014015238\n",
      "Epoch 106, Loss: 0.23726180132399213\n",
      "Epoch 107, Loss: 0.24177448229586823\n",
      "Epoch 108, Loss: 0.24579909824310464\n",
      "Epoch 109, Loss: 0.25413946490338507\n",
      "Epoch 110, Loss: 0.22803118079900742\n",
      "Epoch 111, Loss: 0.23828531484654608\n",
      "Epoch 112, Loss: 0.21344219750546395\n",
      "Epoch 113, Loss: 0.2222033401435994\n",
      "Epoch 114, Loss: 0.22199082691618738\n",
      "Epoch 115, Loss: 0.20539044557099648\n",
      "Epoch 116, Loss: 0.2082417534386858\n",
      "Epoch 117, Loss: 0.20664629673070095\n",
      "Epoch 118, Loss: 0.18614146081691094\n",
      "Epoch 119, Loss: 0.19799247701117334\n",
      "Epoch 120, Loss: 0.18599476776224502\n",
      "Epoch 121, Loss: 0.17646757988853656\n",
      "Epoch 122, Loss: 0.17425948111934864\n",
      "Epoch 123, Loss: 0.17718650479900075\n",
      "Epoch 124, Loss: 0.17015318477407415\n",
      "Epoch 125, Loss: 0.1728471799733791\n",
      "Epoch 126, Loss: 0.1599994236801533\n",
      "Epoch 127, Loss: 0.15924152192917276\n",
      "Epoch 128, Loss: 0.15094604041982204\n",
      "Epoch 129, Loss: 0.14726612066968958\n",
      "Epoch 130, Loss: 0.1480743837483386\n",
      "Epoch 131, Loss: 0.14151958154237015\n",
      "Epoch 132, Loss: 0.15390360276115703\n",
      "Epoch 133, Loss: 0.1507334734531159\n",
      "Epoch 134, Loss: 0.14273494466188105\n",
      "Epoch 135, Loss: 0.13760917031384529\n",
      "Epoch 136, Loss: 0.14176927284991486\n",
      "Epoch 137, Loss: 0.13049174829366358\n",
      "Epoch 138, Loss: 0.12379885092377663\n",
      "Epoch 139, Loss: 0.13274972862068643\n",
      "Epoch 140, Loss: 0.12058980168497309\n",
      "Epoch 141, Loss: 0.13104761376025828\n",
      "Epoch 142, Loss: 0.12620665228113215\n",
      "Epoch 143, Loss: 0.1224620242385154\n",
      "Epoch 144, Loss: 0.1207658107927505\n",
      "Epoch 145, Loss: 0.10673222405479309\n",
      "Epoch 146, Loss: 0.09998996214980775\n",
      "Epoch 147, Loss: 0.10005141858090745\n",
      "Epoch 148, Loss: 0.0969622219179539\n",
      "Epoch 149, Loss: 0.10566063772173638\n",
      "Epoch 150, Loss: 0.09667348195897772\n",
      "Epoch 151, Loss: 0.08879790439250622\n",
      "Epoch 152, Loss: 0.09183809018515526\n",
      "Epoch 153, Loss: 0.09619922603064394\n",
      "Epoch 154, Loss: 0.08260593761471992\n",
      "Epoch 155, Loss: 0.09109568865375316\n",
      "Epoch 156, Loss: 0.08135091279573897\n",
      "Epoch 157, Loss: 0.07892163414904411\n",
      "Epoch 158, Loss: 0.08646724262136093\n",
      "Epoch 159, Loss: 0.07880749363214412\n",
      "Epoch 160, Loss: 0.07993054556402754\n",
      "Epoch 161, Loss: 0.08219784165316439\n",
      "Epoch 162, Loss: 0.07541259283081014\n",
      "Epoch 163, Loss: 0.07462936449558177\n",
      "Epoch 164, Loss: 0.06738264482230583\n",
      "Epoch 165, Loss: 0.06731836608749756\n",
      "Epoch 166, Loss: 0.06745622727148076\n",
      "Epoch 167, Loss: 0.06547622810652916\n",
      "Epoch 168, Loss: 0.06907425222403192\n",
      "Epoch 169, Loss: 0.06183023758708162\n",
      "Epoch 170, Loss: 0.0617759108067827\n",
      "Epoch 171, Loss: 0.057755529682369945\n",
      "Epoch 172, Loss: 0.06084787667273207\n",
      "Epoch 173, Loss: 0.058753106386420575\n",
      "Epoch 174, Loss: 0.062339060682248565\n",
      "Epoch 175, Loss: 0.06343228980264765\n",
      "Epoch 176, Loss: 0.06019721303055895\n",
      "Epoch 177, Loss: 0.06251455359953514\n",
      "Epoch 178, Loss: 0.05634436264951179\n",
      "Epoch 179, Loss: 0.05464675007982457\n",
      "Epoch 180, Loss: 0.04712312046359194\n",
      "Epoch 181, Loss: 0.054086733411284206\n",
      "Epoch 182, Loss: 0.04752671671040515\n",
      "Epoch 183, Loss: 0.044715222090165666\n",
      "Epoch 184, Loss: 0.05218881860058358\n",
      "Epoch 185, Loss: 0.05689127580758105\n",
      "Epoch 186, Loss: 0.04688799317846907\n",
      "Epoch 187, Loss: 0.041881349492580336\n",
      "Epoch 188, Loss: 0.04457462709793385\n",
      "Epoch 189, Loss: 0.041195914644668714\n",
      "Epoch 190, Loss: 0.054997935732628435\n",
      "Epoch 191, Loss: 0.040520882967146156\n",
      "Epoch 192, Loss: 0.039500177758646775\n",
      "Epoch 193, Loss: 0.040936477541765\n",
      "Epoch 194, Loss: 0.04366246682215244\n",
      "Epoch 195, Loss: 0.04355528982395821\n",
      "Epoch 196, Loss: 0.042705114811975906\n",
      "Epoch 197, Loss: 0.07739376344103763\n",
      "Epoch 198, Loss: 0.060351519112257244\n",
      "Epoch 199, Loss: 0.03962472202058168\n",
      "Epoch 200, Loss: 0.03599081838384588\n",
      "Epoch 201, Loss: 0.03494934529620916\n",
      "Epoch 202, Loss: 0.032891834194355825\n",
      "Epoch 203, Loss: 0.030972193074511722\n",
      "Epoch 204, Loss: 0.03153479727421035\n",
      "Epoch 205, Loss: 0.031148228893413187\n",
      "Epoch 206, Loss: 0.034104426609391864\n",
      "Epoch 207, Loss: 0.03536718303060278\n",
      "Epoch 208, Loss: 0.03082993850508269\n",
      "Epoch 209, Loss: 0.03013693214334706\n",
      "Epoch 210, Loss: 0.03151339369806203\n",
      "Epoch 211, Loss: 0.02846781637995167\n",
      "Epoch 212, Loss: 0.03082757087503659\n",
      "Epoch 213, Loss: 0.03325652097292403\n",
      "Epoch 214, Loss: 0.032238186733361256\n",
      "Epoch 215, Loss: 0.028293236713618674\n",
      "Epoch 216, Loss: 0.031654510627243115\n",
      "Epoch 217, Loss: 0.033076512429149864\n",
      "Epoch 218, Loss: 0.02701340362112573\n",
      "Epoch 219, Loss: 0.030901447830206537\n",
      "Epoch 220, Loss: 0.025265635823474287\n",
      "Epoch 221, Loss: 0.027398604176383703\n",
      "Epoch 222, Loss: 0.03216510965548297\n",
      "Epoch 223, Loss: 0.08545793751452832\n",
      "Epoch 224, Loss: 0.055877100081836925\n",
      "Epoch 225, Loss: 0.027592526847853304\n",
      "Epoch 226, Loss: 0.02502861712127924\n",
      "Epoch 227, Loss: 0.030725408186937902\n",
      "Epoch 228, Loss: 0.02577432219256112\n",
      "Epoch 229, Loss: 0.02190653991667514\n",
      "Epoch 230, Loss: 0.02670037688964859\n",
      "Epoch 231, Loss: 0.023541842508030698\n",
      "Epoch 232, Loss: 0.02519284903091636\n",
      "Epoch 233, Loss: 0.024554762602882817\n",
      "Epoch 234, Loss: 0.02446644330159464\n",
      "Epoch 235, Loss: 0.02633465963237463\n",
      "Epoch 236, Loss: 0.027330545114075885\n",
      "Epoch 237, Loss: 0.02240398797662334\n",
      "Epoch 238, Loss: 0.027298558473666298\n",
      "Epoch 239, Loss: 0.02378174388147098\n",
      "Epoch 240, Loss: 0.024314068564946983\n",
      "Epoch 241, Loss: 0.021579206267252883\n",
      "Epoch 242, Loss: 0.024133495510892666\n",
      "Epoch 243, Loss: 0.02336596969039516\n",
      "Epoch 244, Loss: 0.02873851477782777\n",
      "Epoch 245, Loss: 0.02608659598936743\n",
      "Epoch 246, Loss: 0.02159560803680661\n",
      "Epoch 247, Loss: 0.02187878523576767\n",
      "Epoch 248, Loss: 0.021296913398707165\n",
      "Epoch 249, Loss: 0.019478730421434057\n",
      "Epoch 250, Loss: 0.020514852515956822\n",
      "Epoch 251, Loss: 0.02255084750024562\n",
      "Epoch 252, Loss: 0.020549663530781546\n",
      "Epoch 253, Loss: 0.019137443013568508\n",
      "Epoch 254, Loss: 0.025318775950197842\n",
      "Epoch 255, Loss: 0.026766800034315662\n",
      "Epoch 256, Loss: 0.020695445940215537\n",
      "Epoch 257, Loss: 0.021387435386868867\n",
      "Epoch 258, Loss: 0.020003067408787444\n",
      "Epoch 259, Loss: 0.019741039118788977\n",
      "Epoch 260, Loss: 0.019373047542064748\n",
      "Epoch 261, Loss: 0.020639155624791027\n",
      "Epoch 262, Loss: 0.019103650687302045\n",
      "Epoch 263, Loss: 0.025859488907488102\n",
      "Epoch 264, Loss: 0.038607975983239236\n",
      "Epoch 265, Loss: 0.03486404314320138\n",
      "Epoch 266, Loss: 0.025048576840298608\n",
      "Epoch 267, Loss: 0.019731543125941397\n",
      "Epoch 268, Loss: 0.01982488228067597\n",
      "Epoch 269, Loss: 0.022596552789686843\n",
      "Epoch 270, Loss: 0.022690610722341437\n",
      "Epoch 271, Loss: 0.017255351165349177\n",
      "Epoch 272, Loss: 0.01924078804539873\n",
      "Epoch 273, Loss: 0.01946574596668336\n",
      "Epoch 274, Loss: 0.020298815812875935\n",
      "Epoch 275, Loss: 0.018033017785823408\n",
      "Epoch 276, Loss: 0.018853329919635297\n",
      "Epoch 277, Loss: 0.019049871787230704\n",
      "Epoch 278, Loss: 0.02284402917436463\n",
      "Epoch 279, Loss: 0.01907000076422032\n",
      "Epoch 280, Loss: 0.017504518138284377\n",
      "Epoch 281, Loss: 0.017881204027365497\n",
      "Epoch 282, Loss: 0.018033005257553244\n",
      "Epoch 283, Loss: 0.01860645846364663\n",
      "Epoch 284, Loss: 0.024158033215381364\n",
      "Epoch 285, Loss: 0.025487884096404973\n",
      "Epoch 286, Loss: 0.022061036037717092\n",
      "Epoch 287, Loss: 0.017692713270043122\n",
      "Epoch 288, Loss: 0.019033661087420075\n",
      "Epoch 289, Loss: 0.019839884054430938\n",
      "Epoch 290, Loss: 0.018353941776056554\n",
      "Epoch 291, Loss: 0.024450490345306535\n",
      "Epoch 292, Loss: 0.1039733672415481\n",
      "Epoch 293, Loss: 0.05156870399344158\n",
      "Epoch 294, Loss: 0.016902269354644926\n",
      "Epoch 295, Loss: 0.018570580420658945\n",
      "Epoch 296, Loss: 0.01461946499098013\n",
      "Epoch 297, Loss: 0.016199639235484474\n",
      "Epoch 298, Loss: 0.01606023622716361\n",
      "Epoch 299, Loss: 0.014527332751040763\n",
      "Epoch 300, Loss: 0.01406988832960896\n",
      "Epoch 301, Loss: 0.013955342514004479\n",
      "Epoch 302, Loss: 0.014664176638495731\n",
      "Epoch 303, Loss: 0.014365603089669441\n",
      "Epoch 304, Loss: 0.019953997790853075\n",
      "Epoch 305, Loss: 0.017853657883453243\n",
      "Epoch 306, Loss: 0.015371639078403724\n",
      "Epoch 307, Loss: 0.016116062525976846\n",
      "Epoch 308, Loss: 0.015008615632046094\n",
      "Epoch 309, Loss: 0.01582415169600318\n",
      "Epoch 310, Loss: 0.014521400347609627\n",
      "Epoch 311, Loss: 0.014253832533598897\n",
      "Epoch 312, Loss: 0.015074670854757758\n",
      "Epoch 313, Loss: 0.021909452702334905\n",
      "Epoch 314, Loss: 0.02494840102983599\n",
      "Epoch 315, Loss: 0.016242321838248284\n",
      "Epoch 316, Loss: 0.014987040860974726\n",
      "Epoch 317, Loss: 0.013720665077500522\n",
      "Epoch 318, Loss: 0.014721177771013784\n",
      "Epoch 319, Loss: 0.01392937732731646\n",
      "Epoch 320, Loss: 0.017585177138011825\n",
      "Epoch 321, Loss: 0.014723571554876547\n",
      "Epoch 322, Loss: 0.021832771738000373\n",
      "Epoch 323, Loss: 0.07268597325805495\n",
      "Epoch 324, Loss: 0.11129262478665468\n",
      "Epoch 325, Loss: 0.0175477184226459\n",
      "Epoch 326, Loss: 0.014382120157453292\n",
      "Epoch 327, Loss: 0.014241539035983224\n",
      "Epoch 328, Loss: 0.014969902849964243\n",
      "Epoch 329, Loss: 0.013685390189725985\n",
      "Epoch 330, Loss: 0.013842685049043056\n",
      "Epoch 331, Loss: 0.01482981412939014\n",
      "Epoch 332, Loss: 0.016559059674693073\n",
      "Epoch 333, Loss: 0.01487297719304866\n",
      "Epoch 334, Loss: 0.012412751519537352\n",
      "Epoch 335, Loss: 0.015034923414164719\n",
      "Epoch 336, Loss: 0.015359284891251554\n",
      "Epoch 337, Loss: 0.014278657070579046\n",
      "Epoch 338, Loss: 0.01460915273600357\n",
      "Epoch 339, Loss: 0.014001370444794761\n",
      "Epoch 340, Loss: 0.013617750715484168\n",
      "Epoch 341, Loss: 0.014662902856721206\n",
      "Epoch 342, Loss: 0.013425167506322899\n",
      "Epoch 343, Loss: 0.014175661220314337\n",
      "Epoch 344, Loss: 0.016133910243181473\n",
      "Epoch 345, Loss: 0.01464037207807315\n",
      "Epoch 346, Loss: 0.01539807194507027\n",
      "Epoch 347, Loss: 0.03218539324688151\n",
      "Epoch 348, Loss: 0.015148270815769409\n",
      "Epoch 349, Loss: 0.015106717984073181\n",
      "Epoch 350, Loss: 0.016166388156249167\n",
      "Epoch 351, Loss: 0.012330073511545012\n",
      "Epoch 352, Loss: 0.015067621515667502\n",
      "Epoch 353, Loss: 0.014500933139525513\n",
      "Epoch 354, Loss: 0.013154163731242273\n",
      "Epoch 355, Loss: 0.01446668863385678\n",
      "Epoch 356, Loss: 0.024894427503161926\n",
      "Epoch 357, Loss: 0.01605639549001972\n",
      "Epoch 358, Loss: 0.014807581123796867\n",
      "Epoch 359, Loss: 0.017429954094931165\n",
      "Epoch 360, Loss: 0.01413913640104472\n",
      "Epoch 361, Loss: 0.017863257537952603\n",
      "Epoch 362, Loss: 0.01398658288266272\n",
      "Epoch 363, Loss: 0.015205098925396166\n",
      "Epoch 364, Loss: 0.018696502460404597\n",
      "Epoch 365, Loss: 0.020072879413380585\n",
      "Epoch 366, Loss: 0.020134235474657505\n",
      "Epoch 367, Loss: 0.013260341772036825\n",
      "Epoch 368, Loss: 0.014429604270535105\n",
      "Epoch 369, Loss: 0.013787544287860077\n",
      "Epoch 370, Loss: 0.01550627791600183\n",
      "Epoch 371, Loss: 0.01613669163309672\n",
      "Epoch 372, Loss: 0.012523658370519889\n",
      "Epoch 373, Loss: 0.01472630767161621\n",
      "Epoch 374, Loss: 0.014271686398523284\n",
      "Epoch 375, Loss: 0.015531437887631833\n",
      "Epoch 376, Loss: 0.015594857130595979\n",
      "Epoch 377, Loss: 0.015515880230715142\n",
      "Epoch 378, Loss: 0.06692776711598197\n",
      "Epoch 379, Loss: 0.06478556204448513\n",
      "Epoch 380, Loss: 0.018756755553384092\n",
      "Epoch 381, Loss: 0.015441003119315398\n",
      "Epoch 382, Loss: 0.01207154349453668\n",
      "Epoch 383, Loss: 0.013960761862727714\n",
      "Epoch 384, Loss: 0.014485750754829496\n",
      "Epoch 385, Loss: 0.014795889483487352\n",
      "Epoch 386, Loss: 0.014186282022955252\n",
      "Epoch 387, Loss: 0.01369914860573617\n",
      "Epoch 388, Loss: 0.016816114001174238\n",
      "Epoch 389, Loss: 0.013548753177251746\n",
      "Epoch 390, Loss: 0.014033836069853698\n",
      "Epoch 391, Loss: 0.016151922135712937\n",
      "Epoch 392, Loss: 0.01278667609910778\n",
      "Epoch 393, Loss: 0.015850102437283606\n",
      "Epoch 394, Loss: 0.013244391451193455\n",
      "Epoch 395, Loss: 0.012366078179725942\n",
      "Epoch 396, Loss: 0.015114205057158115\n",
      "Epoch 397, Loss: 0.016249702797528912\n",
      "Epoch 398, Loss: 0.014280477925104665\n",
      "Epoch 399, Loss: 0.014268313098619593\n",
      "Epoch 400, Loss: 0.01382924409266165\n",
      "Epoch 401, Loss: 0.014447830015535843\n",
      "Epoch 402, Loss: 0.01763929742972981\n",
      "Epoch 403, Loss: 0.018921476248849895\n",
      "Epoch 404, Loss: 0.017803209115493487\n",
      "Epoch 405, Loss: 0.013368978875966624\n",
      "Epoch 406, Loss: 0.015091616491925842\n",
      "Epoch 407, Loss: 0.03668854624389651\n",
      "Epoch 408, Loss: 0.0320473993593391\n",
      "Epoch 409, Loss: 0.01986261708364683\n",
      "Epoch 410, Loss: 0.015527513130825567\n",
      "Epoch 411, Loss: 0.01280900154837427\n",
      "Epoch 412, Loss: 0.014146068039290766\n",
      "Epoch 413, Loss: 0.014728635018245574\n",
      "Epoch 414, Loss: 0.014269616229439829\n",
      "Epoch 415, Loss: 0.013067065978402946\n",
      "Epoch 416, Loss: 0.014473651103586196\n",
      "Epoch 417, Loss: 0.014737246193962687\n",
      "Epoch 418, Loss: 0.07234361589430495\n",
      "Epoch 419, Loss: 0.05813058694586792\n",
      "Epoch 420, Loss: 0.01519390581017162\n",
      "Epoch 421, Loss: 0.01190220060302539\n",
      "Epoch 422, Loss: 0.01192431760941969\n",
      "Epoch 423, Loss: 0.01112086025185566\n",
      "Epoch 424, Loss: 0.011216229408672278\n",
      "Epoch 425, Loss: 0.01076080390757465\n",
      "Epoch 426, Loss: 0.012776283626543714\n",
      "Epoch 427, Loss: 0.014022069145984788\n",
      "Epoch 428, Loss: 0.011517274324742562\n",
      "Epoch 429, Loss: 0.013179226072524931\n",
      "Epoch 430, Loss: 0.014547688604154169\n",
      "Epoch 431, Loss: 0.014622411772390789\n",
      "Epoch 432, Loss: 0.012395170418188927\n",
      "Epoch 433, Loss: 0.011696047602875277\n",
      "Epoch 434, Loss: 0.01248671575895253\n",
      "Epoch 435, Loss: 0.013146040646577293\n",
      "Epoch 436, Loss: 0.012520829502968712\n",
      "Epoch 437, Loss: 0.013399464259062834\n",
      "Epoch 438, Loss: 0.014103577404154187\n",
      "Epoch 439, Loss: 0.013453408641037234\n",
      "Epoch 440, Loss: 0.01612539012034602\n",
      "Epoch 441, Loss: 0.013346038912323879\n",
      "Epoch 442, Loss: 0.011104381085928292\n",
      "Epoch 443, Loss: 0.013067230043506765\n",
      "Epoch 444, Loss: 0.014700459431549733\n",
      "Epoch 445, Loss: 0.019901342816314005\n",
      "Epoch 446, Loss: 0.030033255766264776\n",
      "Epoch 447, Loss: 0.015824647013474493\n",
      "Epoch 448, Loss: 0.016512739038629893\n",
      "Epoch 449, Loss: 0.013285261960303846\n",
      "Epoch 450, Loss: 0.014160470004153854\n",
      "Epoch 451, Loss: 0.016688523596113034\n",
      "Epoch 452, Loss: 0.01587761095056231\n",
      "Epoch 453, Loss: 0.017301583774090606\n",
      "Epoch 454, Loss: 0.012216484013132434\n",
      "Epoch 455, Loss: 0.01528681130668624\n",
      "Epoch 456, Loss: 0.013616889078625814\n",
      "Epoch 457, Loss: 0.013867995414407329\n",
      "Epoch 458, Loss: 0.012323471864913293\n",
      "Epoch 459, Loss: 0.012958549421300437\n",
      "Epoch 460, Loss: 0.01381170920695753\n",
      "Epoch 461, Loss: 0.01336278303333142\n",
      "Epoch 462, Loss: 0.015064279058393328\n",
      "Epoch 463, Loss: 0.013793807172097583\n",
      "Epoch 464, Loss: 0.01574997736284747\n",
      "Epoch 465, Loss: 0.012342260274887482\n",
      "Epoch 466, Loss: 0.015629291004362576\n",
      "Epoch 467, Loss: 0.015072226809199028\n",
      "Epoch 468, Loss: 0.015450214193093571\n",
      "Epoch 469, Loss: 0.0144390922332024\n",
      "Epoch 470, Loss: 0.017673491408671946\n",
      "Epoch 471, Loss: 0.03462841690696301\n",
      "Epoch 472, Loss: 0.020602329590783872\n",
      "Epoch 473, Loss: 0.012030689905278702\n",
      "Epoch 474, Loss: 0.013468114283936534\n",
      "Epoch 475, Loss: 0.014851034161922067\n",
      "Epoch 476, Loss: 0.015612847376069886\n",
      "Epoch 477, Loss: 0.017198241734381843\n",
      "Epoch 478, Loss: 0.015906882634574668\n",
      "Epoch 479, Loss: 0.03583683515279653\n",
      "Epoch 480, Loss: 0.06632671616674263\n",
      "Epoch 481, Loss: 0.0149544883269242\n",
      "Epoch 482, Loss: 0.011567847087404671\n",
      "Epoch 483, Loss: 0.012281084732905507\n",
      "Epoch 484, Loss: 0.011150080716841794\n",
      "Epoch 485, Loss: 0.013339219761001778\n",
      "Epoch 486, Loss: 0.011688140152824765\n",
      "Epoch 487, Loss: 0.012771458709315259\n",
      "Epoch 488, Loss: 0.01084116164426458\n",
      "Epoch 489, Loss: 0.011356456152614245\n",
      "Epoch 490, Loss: 0.014373334980460796\n",
      "Epoch 491, Loss: 0.01459900363586209\n",
      "Epoch 492, Loss: 0.014385241444192906\n",
      "Epoch 493, Loss: 0.01239498961157106\n",
      "Epoch 494, Loss: 0.014235140381757407\n",
      "Epoch 495, Loss: 0.07522287085265951\n",
      "Epoch 496, Loss: 0.013530804504432338\n",
      "Epoch 497, Loss: 0.011689011640400883\n",
      "Epoch 498, Loss: 0.014637314745452889\n",
      "Epoch 499, Loss: 0.013218594705091511\n",
      "Epoch 500, Loss: 0.013142185158125977\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    normalmodel.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = normalmodel(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 59.88%\n",
      "Test Loss: 2.4268\n",
      "Confusion Matrix:\n",
      "[[25 11  5 11]\n",
      " [ 4 35  7  3]\n",
      " [ 1  6 18  2]\n",
      " [ 6  6  5 22]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "normalmodel.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "total_loss = 0.0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Define your loss function\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # images, labels = images.to(device), labels.to(device)\n",
    "        outputs = normalmodel(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "average_loss = total_loss / len(test_loader)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "# print(all_labels)\n",
    "# print(all_predictions)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "print(f'Test Loss: {average_loss:.4f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now show the pretrained model effect which I found very impressive ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3818558378422514\n",
      "Epoch 2, Loss: 0.9493148022509635\n",
      "Epoch 3, Loss: 0.7353040335026193\n",
      "Epoch 4, Loss: 0.5752794882084461\n",
      "Epoch 5, Loss: 0.47486936855823436\n",
      "Epoch 6, Loss: 0.3797557791496845\n",
      "Epoch 7, Loss: 0.29349237395093797\n",
      "Epoch 8, Loss: 0.25419414550700087\n",
      "Epoch 9, Loss: 0.2060940994861278\n",
      "Epoch 10, Loss: 0.17979888554583204\n",
      "Epoch 11, Loss: 0.14561256052965812\n",
      "Epoch 12, Loss: 0.1338170684398489\n",
      "Epoch 13, Loss: 0.0993003988678151\n",
      "Epoch 14, Loss: 0.09962723594396672\n",
      "Epoch 15, Loss: 0.08607058294434497\n",
      "Epoch 16, Loss: 0.07561213507297192\n",
      "Epoch 17, Loss: 0.06630370476619994\n",
      "Epoch 18, Loss: 0.059379578587856696\n",
      "Epoch 19, Loss: 0.05761527979469046\n",
      "Epoch 20, Loss: 0.04576109595438267\n",
      "Epoch 21, Loss: 0.05056770252896116\n",
      "Epoch 22, Loss: 0.04106190467768527\n",
      "Epoch 23, Loss: 0.032671037684888285\n",
      "Epoch 24, Loss: 0.03462227495347566\n",
      "Epoch 25, Loss: 0.03621509839641921\n",
      "Epoch 26, Loss: 0.03144850732481226\n",
      "Epoch 27, Loss: 0.030429314059066646\n",
      "Epoch 28, Loss: 0.03174481512186058\n",
      "Epoch 29, Loss: 0.027744648522360527\n",
      "Epoch 30, Loss: 0.02451168107701109\n",
      "Epoch 31, Loss: 0.025158275463717416\n",
      "Epoch 32, Loss: 0.026691299773990474\n",
      "Epoch 33, Loss: 0.022014179861450447\n",
      "Epoch 34, Loss: 0.023280338413934124\n",
      "Epoch 35, Loss: 0.02285737917322586\n",
      "Epoch 36, Loss: 0.02000806813049031\n",
      "Epoch 37, Loss: 0.019871625514265072\n",
      "Epoch 38, Loss: 0.018002806850609943\n",
      "Epoch 39, Loss: 0.020706312359370132\n",
      "Epoch 40, Loss: 0.018167328817690623\n",
      "Epoch 41, Loss: 0.02475368704131626\n",
      "Epoch 42, Loss: 0.018178578733684534\n",
      "Epoch 43, Loss: 0.01973943485620808\n",
      "Epoch 44, Loss: 0.016888284411756917\n",
      "Epoch 45, Loss: 0.016803349177551236\n",
      "Epoch 46, Loss: 0.01731147374858723\n",
      "Epoch 47, Loss: 0.01659071261925187\n",
      "Epoch 48, Loss: 0.015927771549731334\n",
      "Epoch 49, Loss: 0.01671113917217689\n",
      "Epoch 50, Loss: 0.01620682888476059\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.62%\n",
      "Test Loss: 0.2767\n",
      "Confusion Matrix:\n",
      "[[42  2  1  3]\n",
      " [ 5 41  0  2]\n",
      " [ 1  0 26  0]\n",
      " [ 4  0  1 39]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "total_loss = 0.0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Define your loss function\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "average_loss = total_loss / len(test_loader)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "# print(all_labels)\n",
    "# print(all_predictions)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "print(f'Test Loss: {average_loss:.4f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
